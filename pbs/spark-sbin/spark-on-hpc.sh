#!/bin/bash

set -e

if [ -z "$SPARK_JOB_DIR" -o -z "$SPARK_HOME" ]; then
   echo "spark-on-hpc.sh requires environment variables SPARK_HOME and SPARK_JOB_DIR" 2>&1
   exit 1
fi

if [ ! -d "$SPARK_JOB_DIR" ]; then
   echo "Cannot find SPARK_JOB_DIR" 2>&1 
   exit 1
fi

SPARK_JOB_DIR=$(readlink -e "$SPARK_JOB_DIR")
SPARK_HOME=$(readlink -e "$SPARK_HOME")

SPARK_CONF_DIR=$SPARK_JOB_DIR/conf
SPARK_CONF=$SPARK_CONF_DIR/spark-defaults.conf
SPARK_ENV=$SPARK_CONF_DIR/spark-env.sh

if [ -f "$SPARK_CONF_DIR/slaves" ]; then
   SLAVES=`cat $SPARK_CONF_DIR/slaves`
fi

case "$1" in 
   start)
      if [ ! -f "$SPARK_ENV" ]; then
         echo "No $SPARK_ENV found. Run spark-on-hpc.sh config first." >&2
         exit 1
      fi

      . $SPARK_ENV
      echo "Use spark config $SPARK_ENV"

      if [ -z "$SPARK_MASTER_PORT" ] || [ "$SPARK_MASTER_PORT" = "RANDOM" ]; then
         RANDOM_PORT_CMD="python -c 'import socket; s1=socket.socket(); s1.bind((\"\", 0)); s2=socket.socket(); s2.bind((\"\",0)); print(\"%d %d\" % (s1.getsockname()[1],s2.getsockname()[1])); s1.close(); s2.close()'"
         RANDOM_PORTS=`ssh $SPARK_MASTER_IP $RANDOM_PORT_CMD`

         read SPARK_MASTER_PORT SPARK_MASTER_WEBUI_PORT <<< $(echo $RANDOM_PORTS)
         if [ -z "$SPARK_MASTER_PORT" ]; then
            echo "Cannot find a random port" >&2
            exit 1
         fi

         sed -i -e "s/^SPARK_MASTER_PORT=.*$/SPARK_MASTER_PORT=$SPARK_MASTER_PORT/" -e "s/^SPARK_MASTER_WEBUI_PORT=.*$/SPARK_MASTER_WEBUI_PORT=$SPARK_MASTER_WEBUI_PORT/" $SPARK_ENV
      fi

      echo "Master URL =  spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT"
      echo "Web UI URL =  http://$SPARK_MASTER_IP:$SPARK_MASTER_WEBUI_PORT"
      echo 
      ssh $SPARK_MASTER_IP ${SPARK_HOME}/sbin/spark-daemon-on-hpc.sh --config $SPARK_CONF_DIR start org.apache.spark.deploy.master.Master 1 --ip $SPARK_MASTER_IP --port $SPARK_MASTER_PORT --webui-port $SPARK_MASTER_WEBUI_PORT 2>&1 | sed "s/^/$SPARK_MASTER_IP: /"

      SLAVE_NUM=1
      for slave in $SLAVES; do
         ssh $slave ${SPARK_HOME}/sbin/spark-daemon-on-hpc.sh --config $SPARK_CONF_DIR start org.apache.spark.deploy.worker.Worker $SLAVE_NUM spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT 2>&1 | sed "s/^/$slave: /" &
         (( SLAVE_NUM++ ))   
      done

      wait

      ;;

   stop)
      if [ ! -f "$SPARK_ENV" ]; then
         echo "No $SPARK_ENV found. Run spark-on-hpc.sh config first." >&2
         exit 1
      fi

      . $SPARK_ENV
      echo "Use spark config $SPARK_ENV"

      SLAVE_NUM=1
      for slave in $SLAVES; do
         ssh $slave ${SPARK_HOME}/sbin/spark-daemon-on-hpc.sh --config $SPARK_CONF_DIR stop org.apache.spark.deploy.worker.Worker $SLAVE_NUM 2>&1 | sed "s/^/$slave: /" &
         (( SLAVE_NUM++ ))   
      done

      ssh $SPARK_MASTER_IP ${SPARK_HOME}/sbin/spark-daemon-on-hpc.sh --config $SPARK_CONF_DIR stop org.apache.spark.deploy.master.Master 1 2>&1 | sed "s/^/$SPARK_MASTER_IP: /" &

      sed -i -e "s/^SPARK_MASTER_PORT=.*$/SPARK_MASTER_PORT=RANDOM/" -e "s/^SPARK_MASTER_WEBUI_PORT=.*$/SPARK_MASTER_WEBUI_PORT=RANDOM/" $SPARK_ENV

      wait
      ;;

   config)
      GEN_HEADER="#--------------Generated by spark-on-hpc.sh -----------------#"
      GEN_FOOTER="#------------------------------------------------------------#"

      if [ -f "$SPARK_ENV" ] && (grep -q "$GEN_HEADER" "$SPARK_ENV"); then
         echo "Existing spark config in $SPARK_JOB_DIR/conf is found, make sure no spark cluster is running or remove the configuration first!!" >&2
         exit 1
      fi

      [ -f "$PBS_NODEFILE" ] || { echo "No PBS_NODEFILE found, should run under qsub" >&2 ; exit 1; }

      if [ -z "$PBS_NUM_PPN" ]; then
         echo "PBS_NUM_PPN is not set!! Torque version may be too old. Set PBS_NUM_PPN manually to ppn" >&2
         exit 1
      fi

      nodes=($( cat "$PBS_NODEFILE" | awk -v N=$PBS_NUM_PPN '{if (++count%N==0) print $0}' | sort ))
      nnodes=${#nodes[@]}
      last=$(( $nnodes - 1 ))

      SPARK_WORKER_DIR=$SPARK_JOB_DIR/work
      SPARK_WORKER_CORES=${PBS_NUM_PPN:-1}
      SPARK_LOG_DIR=$SPARK_JOB_DIR/logs
      SPARK_PID_DIR=$SPARK_LOG_DIR
      SPARK_SLAVES=$SPARK_CONF_DIR/slaves

      MAX_MEM=`ulimit -v`
      if [ "$MAX_MEM" = "unlimited" ] || [ -z "$MAX_MEM" ]; then
         echo "WARNING -l vmem not set, use spark memory 1gb by default" >&2
         SPARK_WORKER_MEMORY="1g"
      else 
         SPARK_WORKER_MEMORY="$((($MAX_MEM+1024-1)/1024))m"
      fi

      #
      # If nodes > 1, -l mem is ignored.
      # -l pmem override -l mem.
      # If there is pmem, Torque finds a node with at least pmem*ppn memory.
      # But ulimit = -l pmem, a worker can only see pmem memory.
      # USE -l vmem seems to solve problems
      #
      #if [ ! -z "$PBS_PMEM" ]; then
      #   PMEM=${PBS_PMEM%?}    # trim last character, assuming "b"
      #   PMEM_SIZE=${PMEM%?}   # get number
      #   PMEM_UNIT=${PMEM: -1} # get last character, assuming m or g
      #   SPARK_WORKER_MEMORY="$(($PMEM_SIZE*$PBS_NUM_PPN))$PMEM_UNIT"
      #else
      #   SPARK_WORKER_MEMORY="1g"
      #fi

      SPARK_MASTER_IP=${nodes[0]}

      mkdir -p $SPARK_CONF_DIR
      cat << EOF >> $SPARK_ENV
$GEN_HEADER
SPARK_MASTER_IP=$SPARK_MASTER_IP
SPARK_MASTER_PORT=RANDOM
SPARK_MASTER_WEBUI_PORT=RANDOM
SPARK_WORKER_DIR=$SPARK_WORKER_DIR
SPARK_WORKER_CORES=$SPARK_WORKER_CORES
SPARK_WORKER_MEMORY=$SPARK_WORKER_MEMORY
SPARK_LOG_DIR=$SPARK_LOG_DIR
SPARK_PID_DIR=$SPARK_PID_DIR
$GEN_FOOTER
EOF

      SECRET=`date | md5sum | head -c6`
      cat << EOF >> $SPARK_CONF
$GEN_HEADER
spark.eventLog.enabled           true
spark.eventLog.dir               file://$SPARK_LOG_DIR

spark.executor.memory		$SPARK_WORKER_MEMORY
spark.driver.memory		$SPARK_WORKER_MEMORY

spark.authenticate              true
spark.authenticate.secret       $SECRET
$GEN_FOOTER
EOF

      printf '%s\n' ${nodes[@]:1} > $SPARK_SLAVES
      ;;

   vars) 
      if [ ! -f "$SPARK_ENV" ] || ! (grep -q "$GEN_HEADER" "$SPARK_ENV"); then
         echo "$SPARK_ENV not found, run spark-on-hpc.sh config first" >&2
         exit 1
      fi
      # Promote to environment variables but doesn't work unless the calling script does this too.
      set -a   
      . $SPARK_ENV
      set +a
      export SPARK_CONF_DIR

      function clean_up {
         echo "Got KILL Signal"
         $SPARK_HOME/sbin/spark-on-hpc.sh stop
         exit 1
      }
      trap clean_up SIGTERM
      echo "Set SIGTERM Handler"
      ;;

   *)
      echo "Usage: spark-on-hpc.sh {config|start|stop}" >&2
      echo "       . ./spark-on-hpc.sh vars (notice the dot)" >&2
      exit 1
      ;;

esac

# Make sure to set this before exit the script normally (exit 0)
set +e
